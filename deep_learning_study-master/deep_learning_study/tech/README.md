# 학습관련 기술들

## 매개변수 갱신

신경망의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는것.=>신경망 최적하는 매우 어려운 문제

### 학률적 경사 하강법(sgd)

    w <- w - n(σL/σw)

sgd는 가장 크게 기울어진 방향으로 가는 것. 기울 어진 방향으로 일정거리 만큼 가는 단순한 방법.

단점 : 비등방성 함수(방향에 따라 성질, 기울기가 달라지는 함수)에서는 탐생 경로가 비효율적

### 모맨텀(Momenturm)
    v <- av - n(σL/σw) (a는 대개 0.9)
    w <- w+v
v는 물리에서 말하는 속도.첫째 식은 기울기 방향으로 힘을 받아 물체가 가속된다는 물리 법칙을 나타냄.
두번 째 식은 공이 기울기를 따라 구르는 듯 움직임을 보여준다.
av는 아무런 힘을 받지 않을 때 서서히 하강 시키는 역할을 한다. 마찰이라고 보면 됨.

### AdaGrad
신경망 학습에서는 학습률 값이 중요.
학습률을 정하는 기술로 '학습률 감소'가 있음.
학습을 진행하면서 학습률을 점차 줄여간다.
가장 간단한 방법은 매개변수 전체의 학습률 값을 일괄적으로 낮추는 것.
AdaGrad는 각각의 매개변수에 맞춤형 값을 만들어 줌
    h <- h + (σL/σw)(σL/σw)    #여기서는 각각의 원소를 곱하는 것.
    w <- w +n(1/sqrt(h))(σL/σw)
매개변수를 갱신 할때, (1/sqrt(h))를 곱해 학습률을 조정함.

### Adam

AdaGrad + Mometurm라고 보면 됨.

## 가중치의 초깃값
가중치의 초깃값을 무엇을 설정하느냐가 신경망의 성패를 가를 수 있음.

### 은닉층의 활성화 값 분포
활성화 값: 활성화 함수의 출력 데이터
각 층의 활성화 값은 적당히 고루 분포 되어야 함.
치우친 데이터가 흐르면 기울기 소실이나 포현력 제한 문제에 빠져 학습이 잘 안됨.
Xavier 초깃값 : 초깃값의 표준편차가 1/sqrt(h)가 되도록 설정. (h는 앞 층의 노드수)


### ReLU를 사용할 때의 초깃값
Xavier은 선형인 것을 전제로 이끔.
ReLU를 사용 할때는 'He 초깃값을' 이용.
표준편차가 2/sqrt(h) 인 정규분포 사용.
## 배치 정규화
학습  속도 개선
초깃값에 크게 의존 안함
오버피팅 억제
각 층에서의 활성화 값이 적당히 분포되도록 조정.
=> 데이터 분포를 정규화 하는 '배치 정규화 계층'을 신경망에 삽입
->Affine-> Batch Norm-> ReLU-> ...

## 바른 학습

### 가중치 감소
오버피팅억제용으로 많이 사용
큰 가중치에 대해서는 그에 상응하는 큰 패널티를 부과하여 오버피팅 억제
오버피팅은 가중치 매개변수 값이 커서 발생하는 경우가 많음.

ex) 가중치의 제곱법칙(L2법칙)은 손실 함수에 더함.
->가중지 감소는 1/(2λw^2)이 되고. 이것을 손실함수에 더함.
λ는 정규화의 세기를 조절하는 하이퍼 파라미터. 크게 할수록 큰 가중치에 대한 패널티가 커짐.

### 드롭 아웃
신경망 모델이 복잡해지면 가중치 감소만으로는 부족.
드롭아웃; 뉴런을 임의로 삭제하면서 학습하는 방법. 훈련 때 은닉층의뉴런을 무작위로 삭제. 시험 때는 모든 뉴련에 신호 전달.
          단, 시험 때는 각 뉴런의 출력에 휸련 때 삭제한 비율을 곱하여 출력.
