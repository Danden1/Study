# 오차역전파

가중치 매개변수의 기울기는 수치 미분을 사용했는데, 수치미분은 시간이 오래 걸린다. 오차 역전파는 이를 보완해준다.

### 그래프를 이용한 계산 

O는 100원, ㅁ는 150원, 소비서 10%
 
 100 2 200  650  1.1  715
 
    O--> * --> + --> *    -->
 
   150 3 450 
 
   ㅁ--> * -->

  -->방향으로 계산하는 것이 순방향이다.

계산 그래프의 특징은 '국소적 계산' 이다.
=> 자신과 관계된 정보만으로 다음 결과 출력 가능. 각 노드는 자신과 관련한 계산만 신경쓰면 됨. 이런 계산이 모이면 복잡한 계산이 됨.

### 연쇄법칙

    <-----  f  <----- 
    E(σy/σx)    E

E 에 국소적 미분을 곱하고, 다음 노드로 전달.

합성 함수의 미분에 대한 성질
합성 함수의 미분은 합성함수를 구성 하는 각 함수의 곱으로 나타낼 수 있다.

    z = t^2, t = x+y

    σz/σx = (σz/σt)(σt/σx)

    σz/σt = 2t  , σt/σx = 1
    2t*1 = 2(x+y)    
      x         t          z
    ----->  +  ----->  ^2 ----->
    <-----     <-----     <-----
    (σt/σx) (σz/σt)(σz/σz)     σz/σz
    (σz/σz)
    (σz/σt)


역전파가 하는 일은 연쇄법칙과 같음.

### 덧셈노드
z = x+ y
σz/σx= 1, σz/σy 1

    x <-----
      1*σL/σz  +  <------
    y <-----       σL/σz
      1*σL/σz
      
 ### 곱셈노드
 
 z = xy
 σz/σx = y, σz/σy = x
 
     x <-----      
        y*σL/σz  * <-------   
     y <-----         σL/σz
        x*σL/σz
        
## 활성화 함수 계층

### sigmoid 계층

    y = 1 / (1+exp(-x))
    

     x       -x     exp(-x)  1+exp(x)   1/(1+exp(-x))
    ----> * ----> exp ----> + ---->    / ----->
          -1                1
          
y = 1/x, σy/σx = - 1/x^2
y = exp(x), σy/σx = exp(x)

           -1                      1
     <----  *  <----  exp   <----  +  <----    / <-----
     (σL/σy)   -(σL/σy)      -(σL/σy)  (σL/σy)    (σL/σy)
     y^2        y^2          y^2       y^2
     *exp(-x)   *exp(-x)
     
     
    <-------- sigmoid  <--------
    (σL/σy)             (σL/σy)
    y^2
    exp(-x)
    
    식을 정리하면,
    (σL/σy) = (σL/σy)*y*(1-y)
    
    출력 y만으로 역전파계산 가능.
    
### ReLU계층

    y = x (x>0)
        0 (x<=0)
    
    (σy/σx) = 1 (x>0)
              0 (x<=0)
             x>0                           x<=0 
    <------ ReLU <------           <------ ReLU <-----
     (σL/σy)       (σL/σy)           0           (σL/σy)
  
  
### Affine 계층

신경망의 순전파에서는 행렬의 내적이 사용됬다. 행렬의 내적을 기하학에서는 affine 변환이라고 한다.
     
    X(2,)
    ----->      X'W(3,)      Y(3,)
             dot -----> + ------>
    ----->             B(3,)
    W(2,3)

   σL/σx =(σL/σy)W.T,  σL/σw = X.T(σL/σy)
   .T는 전치행렬. w(i,j) 의 전치행렬은 w(j,i)
      
      1
   <-----
          dot ------ + <------
   <-----     σL/σy(3,) σL/σy(3,)
      2
     
   1: σL/σx =(σL/σy)W.T,  2: σL/σw = X.T(σL/σy)
   
배치의 경우에는 x의 형상이 (N,2)로 됨.


### Softmax-with-Loss 계층

Affine -> RelU -> Affine->ReLU->.... ->Affine ->Softmax

이런 식으로 계층이 구현된다.

Softmax앞의 값을 점수라고 한다.

Softmax-with-Loss함수는

softmax->cross entropy error

로 구성되어있다.
     
     a1                  y1,t1
   
    ---->                 ---->
   
    <----                 <----
   
    y1-t1                  
   
    a2                   y2,t2                      L
   
    ---->      Softmax    ---->     Cross           ----->   
   
    <----                 <----     Entropy error   <-----
   
    y2-t2                                            1
   
    a3                   y3,t3
   
    ---->                 ---->
   
    <----                 <----
   
    y3-t3
    
    
    
    
 역전파의 결과로 (y1 - t1, y2 - t2, y3- t3)로 깔끔하게 출력되고 있다.
 
 출력과 정답레이블의 차분이다.
 
 이것은 일부러 이렇게 설계 된 것이다.
 
 
 ### 오차역전파 구현

소스파일을 보면 OrderDict은 순서가 있는 딕셔너리이다.

5층, 10층, .... 으로 구현하고 싶으면 단순히 계층을 추가하면 된다.

오차역전파를 구현한 학습은 Learning_neural_net/train_neuralnet.py의 numerical.gradient를 gradient로 바꿔주면 된다.
